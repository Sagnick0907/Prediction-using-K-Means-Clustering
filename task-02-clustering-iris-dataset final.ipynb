{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Science & Business Analytics Tasks\n<font size=6>\n    BY - SAGNICK BHAR\n\n\n<font color='blue'>\n<font size=4>\n\n## TASK 2 - Unsupervised Machine Learning - Clustering(Iris DataSet)\n","metadata":{}},{"cell_type":"markdown","source":"## Overview of the Problem set ##\n\n**Problem Statement**: \nFrom the given ‘Iris’ dataset, predict the optimum number of clusters\nand represent it visually.","metadata":{}},{"cell_type":"markdown","source":"## Importing Packages ##\n\nFirst, let's import all the packages that will be needed during this assignment. ","metadata":{}},{"cell_type":"code","source":"# Importing the libraries ---\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler","metadata":{"execution":{"iopub.status.busy":"2021-08-09T12:32:35.312834Z","iopub.execute_input":"2021-08-09T12:32:35.313528Z","iopub.status.idle":"2021-08-09T12:32:35.321194Z","shell.execute_reply.started":"2021-08-09T12:32:35.313472Z","shell.execute_reply":"2021-08-09T12:32:35.319399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the iris dataset ---\n\ndf = datasets.load_iris()\nprint('Data Successfully Imported')\ndf = pd.DataFrame(df.data, columns = df.feature_names)\ndf.head() # See the first 5 rows","metadata":{"execution":{"iopub.status.busy":"2021-08-09T12:32:35.323664Z","iopub.execute_input":"2021-08-09T12:32:35.324193Z","iopub.status.idle":"2021-08-09T12:32:35.351731Z","shell.execute_reply.started":"2021-08-09T12:32:35.324152Z","shell.execute_reply":"2021-08-09T12:32:35.350786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To know number of rows and collumns\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-09T12:32:35.353412Z","iopub.execute_input":"2021-08-09T12:32:35.35375Z","iopub.status.idle":"2021-08-09T12:32:35.359694Z","shell.execute_reply.started":"2021-08-09T12:32:35.353702Z","shell.execute_reply":"2021-08-09T12:32:35.358723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To find if any null value is present\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T12:32:35.361619Z","iopub.execute_input":"2021-08-09T12:32:35.362508Z","iopub.status.idle":"2021-08-09T12:32:35.379599Z","shell.execute_reply.started":"2021-08-09T12:32:35.362416Z","shell.execute_reply":"2021-08-09T12:32:35.378262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To see summary statistics such as the percentiles, mean, std, max, count of the given dataset.\ndf.describe().T","metadata":{"execution":{"iopub.status.busy":"2021-08-09T12:32:35.38339Z","iopub.execute_input":"2021-08-09T12:32:35.383939Z","iopub.status.idle":"2021-08-09T12:32:35.422668Z","shell.execute_reply.started":"2021-08-09T12:32:35.383875Z","shell.execute_reply":"2021-08-09T12:32:35.421298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To find the outliers\ncols = df.columns\nfor i in cols:\n    sns.boxplot(y=df[i])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T12:32:35.425054Z","iopub.execute_input":"2021-08-09T12:32:35.425567Z","iopub.status.idle":"2021-08-09T12:32:36.140441Z","shell.execute_reply.started":"2021-08-09T12:32:35.425514Z","shell.execute_reply":"2021-08-09T12:32:36.138929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From the above boxplot we can say that there are outliers in the column 'sepal width (cm)'","metadata":{}},{"cell_type":"code","source":"# To remove outliers from 'sepal width (cm)'\nq1 = df['sepal width (cm)'].quantile(0.25)\nq3 = df['sepal width (cm)'].quantile(0.75)\niqr = q3 - q1\ndf = df[(df['sepal width (cm)'] >= q1-1.5*iqr) & (df['sepal width (cm)'] <= q3+1.5*iqr)]\ndf.shape # To find out the number of rows and column after outlier treatment","metadata":{"execution":{"iopub.status.busy":"2021-08-09T12:32:36.142363Z","iopub.execute_input":"2021-08-09T12:32:36.142856Z","iopub.status.idle":"2021-08-09T12:32:36.159087Z","shell.execute_reply.started":"2021-08-09T12:32:36.142805Z","shell.execute_reply":"2021-08-09T12:32:36.157896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- After removing the outliers, the number of rows are reduced to 146 from 150","metadata":{}},{"cell_type":"code","source":"# Blocplot for sepal width (cm) after outlier treatment\nsns.boxplot(y=df['sepal width (cm)'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T12:32:36.160967Z","iopub.execute_input":"2021-08-09T12:32:36.161592Z","iopub.status.idle":"2021-08-09T12:32:36.312273Z","shell.execute_reply.started":"2021-08-09T12:32:36.16141Z","shell.execute_reply":"2021-08-09T12:32:36.31111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardizing to avoid bias ---\n\nstandard_scaler = StandardScaler()\ndf_norm = standard_scaler.fit_transform(df)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T12:32:36.313928Z","iopub.execute_input":"2021-08-09T12:32:36.314274Z","iopub.status.idle":"2021-08-09T12:32:36.325761Z","shell.execute_reply.started":"2021-08-09T12:32:36.314238Z","shell.execute_reply":"2021-08-09T12:32:36.32458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Finding the optimal no. of cluster(s) ---\n\nx = df.iloc[:, [0, 1, 2, 3]].values\n\nfrom sklearn.cluster import KMeans\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', \n                    max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(x)\n    wcss.append(kmeans.inertia_)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T12:32:36.327236Z","iopub.execute_input":"2021-08-09T12:32:36.327578Z","iopub.status.idle":"2021-08-09T12:32:36.818608Z","shell.execute_reply.started":"2021-08-09T12:32:36.327545Z","shell.execute_reply":"2021-08-09T12:32:36.817477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ploting elbow curve to find the no. of cluster ---\n\nplt.plot(range(1, 11), wcss)\nplt.xlabel('Values of K') \nplt.ylabel('WCSS')  # Within cluster sum of squares\nplt.title('The Elbow Method using Distortion') \nplt.show() ","metadata":{"execution":{"iopub.status.busy":"2021-08-09T12:32:36.821565Z","iopub.execute_input":"2021-08-09T12:32:36.822018Z","iopub.status.idle":"2021-08-09T12:32:37.006368Z","shell.execute_reply.started":"2021-08-09T12:32:36.821975Z","shell.execute_reply":"2021-08-09T12:32:37.004826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"The elbow method\" got its name from the elbow pattern forming something like above. The optimal clusters are formed where the elbow occurs. This is when the WCSS(Within Cluster Sum of Squares) doesn't decrease with every iteration significantly.\n\nHere we choose the number of clusters as '3'.","metadata":{}},{"cell_type":"markdown","source":"## Creating K-Means Classifier ##\n","metadata":{}},{"cell_type":"code","source":"# Applying kmeans to the dataset ---\n# Creating the kmeans classifier ---\n\nkmeans = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\ny_kmeans = kmeans.fit_predict(x)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T12:32:37.008829Z","iopub.execute_input":"2021-08-09T12:32:37.009249Z","iopub.status.idle":"2021-08-09T12:32:37.052977Z","shell.execute_reply.started":"2021-08-09T12:32:37.009213Z","shell.execute_reply":"2021-08-09T12:32:37.05179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing the cluster data ##\n","metadata":{}},{"cell_type":"code","source":"\n# Visualising the clusters ---\n# Preferably on the first two columns\n\nplt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Iris-setosa')\nplt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Iris-versicolour')\nplt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Iris-virginica')","metadata":{"execution":{"iopub.status.busy":"2021-08-09T12:32:37.054449Z","iopub.execute_input":"2021-08-09T12:32:37.054861Z","iopub.status.idle":"2021-08-09T12:32:37.281196Z","shell.execute_reply.started":"2021-08-09T12:32:37.05482Z","shell.execute_reply":"2021-08-09T12:32:37.279788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the centroids of the clusters ---\n\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'yellow', label = 'Centroids')\n\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T12:32:37.282957Z","iopub.execute_input":"2021-08-09T12:32:37.283385Z","iopub.status.idle":"2021-08-09T12:32:37.497574Z","shell.execute_reply.started":"2021-08-09T12:32:37.283348Z","shell.execute_reply":"2021-08-09T12:32:37.496433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now Combining both the above graphs together ##\n","metadata":{}},{"cell_type":"code","source":"# Visualising the clusters ---\n\nplt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Iris-setosa')\nplt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Iris-versicolour')\nplt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Iris-virginica')\n\n# Plotting centroids of the clusters ---\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'yellow', label = 'Centroids')\n\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T12:32:37.498993Z","iopub.execute_input":"2021-08-09T12:32:37.499295Z","iopub.status.idle":"2021-08-09T12:32:37.765675Z","shell.execute_reply.started":"2021-08-09T12:32:37.499265Z","shell.execute_reply":"2021-08-09T12:32:37.76413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CONCLUSION ##\nThis concludes the application of K-Means Clustering Algorithm and the respective Graphical Representation of Clusters.\n","metadata":{}}]}